"""
Airflow DAG: bcb_indicadores_dag.py
----------------------------------
Extrai sÃ©ries temporais do SGS (Banco Central do Brasil), agrega por mÃªs
(equivalente Ã  mÃ©dia mensal) e salva CSV consolidado.

â€¢ Indicadores capturados
  - SELIC diÃ¡ria (cÃ³d. 11)
  - SELIC 12â€¯m (cÃ³d. 4390)
  - IPCA (cÃ³d. 433)
  - DÃ³lar (cÃ³d. 1)
  - PIB (cÃ³d. 4380)
  - PoupanÃ§a (cÃ³d. 195)

â€¢ Etapas da DAG (3 tasks)
  1. **extrair_dados**  â†’ baixa e normaliza cada sÃ©rie
  2. **agregar_dados**  â†’ calcula mÃ©dia mensal e consolida tabelas
  3. **salvar_resultado** â†’ grava /tmp/indicadores_consolidados.csv e mostra preview

A DAG roda toda segundaâ€‘feira Ã s 08:00 (horÃ¡rio do Airflow) e utiliza
apenas `requests`, `pandas`, `datetime` e operadores nativos do Airflow.
"""

from __future__ import annotations

import json
from datetime import datetime, timedelta
from typing import Dict

import pandas as pd
import requests
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ConfiguraÃ§Ãµes
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SERIES: Dict[str, int] = {
    "selic_diaria": 11,
    "selic_12m": 4390,
    "ipca": 433,
    "dolar": 1,
    "pib": 4380,
    "poupanca": 195,
}

DEFAULT_ARGS = {
    "owner": "airflow",
    "start_date": datetime(2023, 1, 1),
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

DATA_INICIAL = "01/01/2020"  # formato dd/mm/aaaa
CSV_PATH = "/tmp/indicadores_consolidados.csv"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FunÃ§Ãµes das tasks
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extrair_dados(**context):
    """Baixa cada sÃ©rie do SGS e envia DataFrames (JSON) via XCom."""

    data_final = datetime.today().strftime("%d/%m/%Y")
    dfs_raw = {}

    for nome, codigo in SERIES.items():
        url = (
            f"https://api.bcb.gov.br/dados/serie/bcdata.sgs.{codigo}/dados"
            f"?formato=json&dataInicial={DATA_INICIAL}&dataFinal={data_final}"
        )
        print(f"ðŸ“¡ Baixando {nome} ({codigo}) â€¦")
        resp = requests.get(url, timeout=30)
        resp.raise_for_status()

        df = pd.DataFrame(resp.json())
        df["data"] = pd.to_datetime(df["data"], dayfirst=True)
        df["valor"] = pd.to_numeric(df["valor"].str.replace(",", "."), errors="coerce")
        df = df.rename(columns={"valor": nome})
        dfs_raw[nome] = df[["data", nome]]

    # Serializa cada DataFrame como JSON e empurra para XCom
    context["ti"].xcom_push(
        key="series_raw",
        value={k: v.to_json() for k, v in dfs_raw.items()},
    )


def agregar_dados(**context):
    """Converte para DataFrame, agrega por mÃªs e consolida todas as sÃ©ries."""

    series_json = context["ti"].xcom_pull(key="series_raw", task_ids="extrair_dados")
    dfs_mensais = []

    for nome, json_str in series_json.items():
        df = pd.read_json(json_str)
        df["data"] = pd.to_datetime(df["data"], errors="coerce")
        df["ano_mes"] = df["data"].dt.to_period("M")
        df_mensal = df.groupby("ano_mes")[nome].mean().reset_index()
        dfs_mensais.append(df_mensal)

    consolidado = dfs_mensais[0]
    for df in dfs_mensais[1:]:
        consolidado = pd.merge(consolidado, df, on="ano_mes", how="outer")

    consolidado["ano_mes"] = consolidado["ano_mes"].astype(str)
    consolidado = consolidado.sort_values("ano_mes").reset_index(drop=True)

    context["ti"].xcom_push(key="consolidado", value=consolidado.to_json())


def salvar_resultado(**context):
    """Grava CSV consolidado em disco e mostra Ãºltimas linhas no log."""

    consolidado_json = context["ti"].xcom_pull(key="consolidado", task_ids="agregar_dados")
    df = pd.read_json(consolidado_json)
    df.to_csv(CSV_PATH, index=False)
    print("âœ… CSV salvo em", CSV_PATH)
    print(df.tail())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DefiniÃ§Ã£o da DAG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with DAG(
    dag_id="bcb_indicadores_dag",
    description="Extrai e consolida indicadores do SGS do Banco Central",
    default_args=DEFAULT_ARGS,
    schedule_interval="0 8 * * 1",  # segundaâ€‘feira 08:00
    catchup=False,
    tags=["bcb", "indicadores", "etl"],
    max_active_runs=1,
) as dag:

    extrair = PythonOperator(
        task_id="extrair_dados",
        python_callable=extrair_dados,
    )

    agregar = PythonOperator(
        task_id="agregar_dados",
        python_callable=agregar_dados,
    )

    salvar = PythonOperator(
        task_id="salvar_resultado",
        python_callable=salvar_resultado,
    )

    extrair >> agregar >> salvar
